{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First things first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the libraries that will help to clean the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /home/carolina/.local/lib/python3.7/site-packages (2020.2.20)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/carolina/Desktop/IRONHACK/Project files/attacks.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Investigator or Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>25-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Boating</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Oceanside, San Diego County</td>\n",
       "      <td>Paddling</td>\n",
       "      <td>Julie Wolfe</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>White shark</td>\n",
       "      <td>R. Collier, GSAF</td>\n",
       "      <td>2018.06.25-Wolfe.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>18-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>St. Simon Island, Glynn County</td>\n",
       "      <td>Standing</td>\n",
       "      <td>AdysonÂ McNeely</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.18-McNeely.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>6302.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>09-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Habush, Oahu</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>John Denges</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.09-Denges.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>6301.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>08-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Arrawarra Headland</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>B. Myatt, GSAF</td>\n",
       "      <td>2018.06.08-Arrawarra.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>04-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>La Ticla</td>\n",
       "      <td>Free diving</td>\n",
       "      <td>Gustavo Ramos</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark, 3m</td>\n",
       "      <td>A .Kipper</td>\n",
       "      <td>2018.06.04-Ramos.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>6299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018.06.03.b</td>\n",
       "      <td>03-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Flat Rock, Ballina</td>\n",
       "      <td>Kite surfing</td>\n",
       "      <td>Chris</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Daily Telegraph, 6/4/2018</td>\n",
       "      <td>2018.06.03.b-FlatRock.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.03.b</td>\n",
       "      <td>2018.06.03.b</td>\n",
       "      <td>6298.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018.06.03.a</td>\n",
       "      <td>03-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>BRAZIL</td>\n",
       "      <td>Pernambuco</td>\n",
       "      <td>Piedade Beach, Recife</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Jose Ernesto da Silva</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark</td>\n",
       "      <td>Diario de Pernambuco, 6/4/2018</td>\n",
       "      <td>2018.06.03.a-daSilva.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.03.a</td>\n",
       "      <td>2018.06.03.a</td>\n",
       "      <td>6297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018.05.27</td>\n",
       "      <td>27-May-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Lighhouse Point Park, Ponce Inlet, Volusia County</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Lemon shark, 3'</td>\n",
       "      <td>K. McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.05.27-Ponce.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.27</td>\n",
       "      <td>2018.05.27</td>\n",
       "      <td>6296.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018.05.26.b</td>\n",
       "      <td>26-May-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Cocoa Beach, Brevard  County</td>\n",
       "      <td>Walking</td>\n",
       "      <td>Cody High</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Bull shark, 6'</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.05.26.b-High.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.26.b</td>\n",
       "      <td>2018.05.26.b</td>\n",
       "      <td>6295.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018.05.26.a</td>\n",
       "      <td>26-May-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Daytona Beach, Volusia County</td>\n",
       "      <td>Standing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K. McMurray, Tracking Sharks.com</td>\n",
       "      <td>2018.05.26.a-DaytonaBeach.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.26.a</td>\n",
       "      <td>2018.05.26.a</td>\n",
       "      <td>6294.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018.05.24</td>\n",
       "      <td>24-May-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>Cairns Aquarium</td>\n",
       "      <td>Feeding sharks</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Grey reef shark</td>\n",
       "      <td>ABC.net.au ,05/24/2018</td>\n",
       "      <td>2018.05.24-CairnsAquarium.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.24</td>\n",
       "      <td>2018.05.24</td>\n",
       "      <td>6293.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018.05.21</td>\n",
       "      <td>21-May-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Isle of Palms,  Charleston County</td>\n",
       "      <td>Boogie boarding</td>\n",
       "      <td>Trey de Boer</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C. Creswell, GSAF</td>\n",
       "      <td>2018.05.21-deBoer.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.21</td>\n",
       "      <td>2018.05.21</td>\n",
       "      <td>6292.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018.05.13.b</td>\n",
       "      <td>13-May-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Hilton Head Island, Beaufort County</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Jei Turrell</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C. Creswell, GSAF &amp; K. McMurray TrackingSharks...</td>\n",
       "      <td>2018.05.13.b-Turrell.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.13.b</td>\n",
       "      <td>2018.05.13.b</td>\n",
       "      <td>6291.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018.05.13.a</td>\n",
       "      <td>13-May-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>ENGLAND</td>\n",
       "      <td>Cornwall</td>\n",
       "      <td>Off Land's End</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Max Berryman</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Invalid incident</td>\n",
       "      <td>K. McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.05.13.a-Berryman.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.13.a</td>\n",
       "      <td>2018.05.13.a</td>\n",
       "      <td>6290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018.05.00</td>\n",
       "      <td>May 2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Westerm Australia</td>\n",
       "      <td>Dugong Bay</td>\n",
       "      <td>Feeding sharks</td>\n",
       "      <td>Melisa Brunning</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>Tawny nurse shark, 2m</td>\n",
       "      <td>Perth Now, 6/30/2018</td>\n",
       "      <td>2018.05.00-Brunning.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.05.00</td>\n",
       "      <td>2018.05.00</td>\n",
       "      <td>6289.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows Ã 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Case Number         Date    Year        Type    Country  \\\n",
       "0     2018.06.25  25-Jun-2018  2018.0     Boating        USA   \n",
       "1     2018.06.18  18-Jun-2018  2018.0  Unprovoked        USA   \n",
       "2     2018.06.09  09-Jun-2018  2018.0     Invalid        USA   \n",
       "3     2018.06.08  08-Jun-2018  2018.0  Unprovoked  AUSTRALIA   \n",
       "4     2018.06.04  04-Jun-2018  2018.0    Provoked     MEXICO   \n",
       "5   2018.06.03.b  03-Jun-2018  2018.0  Unprovoked  AUSTRALIA   \n",
       "6   2018.06.03.a  03-Jun-2018  2018.0  Unprovoked     BRAZIL   \n",
       "7     2018.05.27  27-May-2018  2018.0  Unprovoked        USA   \n",
       "8   2018.05.26.b  26-May-2018  2018.0  Unprovoked        USA   \n",
       "9   2018.05.26.a  26-May-2018  2018.0  Unprovoked        USA   \n",
       "10    2018.05.24  24-May-2018  2018.0    Provoked  AUSTRALIA   \n",
       "11    2018.05.21  21-May-2018  2018.0  Unprovoked        USA   \n",
       "12  2018.05.13.b  13-May-2018  2018.0  Unprovoked        USA   \n",
       "13  2018.05.13.a  13-May-2018  2018.0     Invalid    ENGLAND   \n",
       "14    2018.05.00     May 2018  2018.0    Provoked  AUSTRALIA   \n",
       "\n",
       "                 Area                                           Location  \\\n",
       "0          California                        Oceanside, San Diego County   \n",
       "1             Georgia                     St. Simon Island, Glynn County   \n",
       "2              Hawaii                                       Habush, Oahu   \n",
       "3     New South Wales                                 Arrawarra Headland   \n",
       "4              Colima                                           La Ticla   \n",
       "5     New South Wales                                 Flat Rock, Ballina   \n",
       "6          Pernambuco                              Piedade Beach, Recife   \n",
       "7             Florida  Lighhouse Point Park, Ponce Inlet, Volusia County   \n",
       "8             Florida                       Cocoa Beach, Brevard  County   \n",
       "9             Florida                      Daytona Beach, Volusia County   \n",
       "10         Queensland                                    Cairns Aquarium   \n",
       "11     South Carolina                  Isle of Palms,  Charleston County   \n",
       "12     South Carolina                Hilton Head Island, Beaufort County   \n",
       "13           Cornwall                                     Off Land's End   \n",
       "14  Westerm Australia                                         Dugong Bay   \n",
       "\n",
       "           Activity                    Name Sex      ...      \\\n",
       "0          Paddling             Julie Wolfe    F     ...       \n",
       "1          Standing         AdysonÂ McNeely     F     ...       \n",
       "2           Surfing             John Denges    M     ...       \n",
       "3           Surfing                    male    M     ...       \n",
       "4       Free diving          Gustavo Ramos     M     ...       \n",
       "5      Kite surfing                 Chris \n",
       "    M     ...       \n",
       "6          Swimming  Jose Ernesto da Silva     M     ...       \n",
       "7           Fishing                    male    M     ...       \n",
       "8           Walking               Cody High    M     ...       \n",
       "9          Standing                    male    M     ...       \n",
       "10   Feeding sharks                    male    M     ...       \n",
       "11  Boogie boarding            Trey de Boer    M     ...       \n",
       "12         Swimming             Jei Turrell    M     ...       \n",
       "13          Fishing            Max Berryman    M     ...       \n",
       "14   Feeding sharks         Melisa Brunning    F     ...       \n",
       "\n",
       "                 Species                              Investigator or Source  \\\n",
       "0             White shark                                   R. Collier, GSAF   \n",
       "1                     NaN                     K.McMurray, TrackingSharks.com   \n",
       "2                     NaN                     K.McMurray, TrackingSharks.com   \n",
       "3               2 m shark                                     B. Myatt, GSAF   \n",
       "4         Tiger shark, 3m                                          A .Kipper   \n",
       "5                     NaN                          Daily Telegraph, 6/4/2018   \n",
       "6             Tiger shark                     Diario de Pernambuco, 6/4/2018   \n",
       "7         Lemon shark, 3'                    K. McMurray, TrackingSharks.com   \n",
       "8          Bull shark, 6'                     K.McMurray, TrackingSharks.com   \n",
       "9                     NaN                   K. McMurray, Tracking Sharks.com   \n",
       "10        Grey reef shark                             ABC.net.au ,05/24/2018   \n",
       "11                    NaN                                  C. Creswell, GSAF   \n",
       "12                    NaN  C. Creswell, GSAF & K. McMurray TrackingSharks...   \n",
       "13       Invalid incident                    K. McMurray, TrackingSharks.com   \n",
       "14  Tawny nurse shark, 2m                               Perth Now, 6/30/2018   \n",
       "\n",
       "                              pdf  \\\n",
       "0            2018.06.25-Wolfe.pdf   \n",
       "1          2018.06.18-McNeely.pdf   \n",
       "2           2018.06.09-Denges.pdf   \n",
       "3        2018.06.08-Arrawarra.pdf   \n",
       "4            2018.06.04-Ramos.pdf   \n",
       "5       2018.06.03.b-FlatRock.pdf   \n",
       "6        2018.06.03.a-daSilva.pdf   \n",
       "7            2018.05.27-Ponce.pdf   \n",
       "8           2018.05.26.b-High.pdf   \n",
       "9   2018.05.26.a-DaytonaBeach.pdf   \n",
       "10  2018.05.24-CairnsAquarium.pdf   \n",
       "11          2018.05.21-deBoer.pdf   \n",
       "12       2018.05.13.b-Turrell.pdf   \n",
       "13      2018.05.13.a-Berryman.pdf   \n",
       "14        2018.05.00-Brunning.pdf   \n",
       "\n",
       "                                         href formula  \\\n",
       "0   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "1   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "2   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "3   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "4   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "5   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "6   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "7   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "8   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "9   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "10  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "11  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "12  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "13  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "14  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "\n",
       "                                                 href Case Number.1  \\\n",
       "0   http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.25   \n",
       "1   http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.18   \n",
       "2   http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.09   \n",
       "3   http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.08   \n",
       "4   http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.04   \n",
       "5   http://sharkattackfile.net/spreadsheets/pdf_di...  2018.06.03.b   \n",
       "6   http://sharkattackfile.net/spreadsheets/pdf_di...  2018.06.03.a   \n",
       "7   http://sharkattackfile.net/spreadsheets/pdf_di...    2018.05.27   \n",
       "8   http://sharkattackfile.net/spreadsheets/pdf_di...  2018.05.26.b   \n",
       "9   http://sharkattackfile.net/spreadsheets/pdf_di...  2018.05.26.a   \n",
       "10  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.05.24   \n",
       "11  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.05.21   \n",
       "12  http://sharkattackfile.net/spreadsheets/pdf_di...  2018.05.13.b   \n",
       "13  http://sharkattackfile.net/spreadsheets/pdf_di...  2018.05.13.a   \n",
       "14  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.05.00   \n",
       "\n",
       "   Case Number.2 original order Unnamed: 22 Unnamed: 23  \n",
       "0     2018.06.25         6303.0         NaN         NaN  \n",
       "1     2018.06.18         6302.0         NaN         NaN  \n",
       "2     2018.06.09         6301.0         NaN         NaN  \n",
       "3     2018.06.08         6300.0         NaN         NaN  \n",
       "4     2018.06.04         6299.0         NaN         NaN  \n",
       "5   2018.06.03.b         6298.0         NaN         NaN  \n",
       "6   2018.06.03.a         6297.0         NaN         NaN  \n",
       "7     2018.05.27         6296.0         NaN         NaN  \n",
       "8   2018.05.26.b         6295.0         NaN         NaN  \n",
       "9   2018.05.26.a         6294.0         NaN         NaN  \n",
       "10    2018.05.24         6293.0         NaN         NaN  \n",
       "11    2018.05.21         6292.0         NaN         NaN  \n",
       "12  2018.05.13.b         6291.0         NaN         NaN  \n",
       "13  2018.05.13.a         6290.0         NaN         NaN  \n",
       "14    2018.05.00         6289.0         NaN         NaN  \n",
       "\n",
       "[15 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25723, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows x Columns\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
       "       'Unnamed: 23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns name\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>No injury to occupant, outrigger canoe and pad...</td>\n",
       "      <td>N</td>\n",
       "      <td>18h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>Minor injury to left thigh</td>\n",
       "      <td>N</td>\n",
       "      <td>14h00  -15h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>Injury to left lower leg from surfboard skeg</td>\n",
       "      <td>N</td>\n",
       "      <td>07h45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Minor injury to lower leg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Lacerations to leg &amp; hand shark PROVOKED INCIDENT</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age                                             Injury Fatal (Y/N)  \\\n",
       "0   57  No injury to occupant, outrigger canoe and pad...           N   \n",
       "1   11                         Minor injury to left thigh           N   \n",
       "2   48       Injury to left lower leg from surfboard skeg           N   \n",
       "3  NaN                          Minor injury to lower leg           N   \n",
       "4  NaN  Lacerations to leg & hand shark PROVOKED INCIDENT           N   \n",
       "\n",
       "            Time  \n",
       "0          18h00  \n",
       "1  14h00  -15h00  \n",
       "2          07h45  \n",
       "3            NaN  \n",
       "4            NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns that are hidden on the df above:\n",
    "\n",
    "df[['Age', 'Injury', 'Fatal (Y/N)', 'Time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this first investigation we can highlight some observations:\n",
    "- The columns \"Unnamed: 22\" and \"Unnamed: 23\" seems to don't have any information. \n",
    "- The columns \"Case Number 1.\" and \"Case Number 2.\" looks like duplicated from the column Case Number.\n",
    "- Some columns such as \"pdf\", \"href formula\", \"href\", \"original order\" might won't be usefull for the analysis.\n",
    "\n",
    "If these obervarions are correct we can erase these columns, but before deleting any data we need to make sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Clean action: Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start to manipulate the data, let's check if we have duplicated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Investigator or Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>25-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Boating</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Oceanside, San Diego County</td>\n",
       "      <td>Paddling</td>\n",
       "      <td>Julie Wolfe</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>White shark</td>\n",
       "      <td>R. Collier, GSAF</td>\n",
       "      <td>2018.06.25-Wolfe.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>18-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>St. Simon Island, Glynn County</td>\n",
       "      <td>Standing</td>\n",
       "      <td>AdysonÂ McNeely</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.18-McNeely.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>6302.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>09-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Habush, Oahu</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>John Denges</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.09-Denges.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>6301.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>08-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Arrawarra Headland</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>B. Myatt, GSAF</td>\n",
       "      <td>2018.06.08-Arrawarra.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>04-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>La Ticla</td>\n",
       "      <td>Free diving</td>\n",
       "      <td>Gustavo Ramos</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark, 3m</td>\n",
       "      <td>A .Kipper</td>\n",
       "      <td>2018.06.04-Ramos.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>6299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Case Number         Date    Year        Type    Country             Area  \\\n",
       "0  2018.06.25  25-Jun-2018  2018.0     Boating        USA       California   \n",
       "1  2018.06.18  18-Jun-2018  2018.0  Unprovoked        USA          Georgia   \n",
       "2  2018.06.09  09-Jun-2018  2018.0     Invalid        USA           Hawaii   \n",
       "3  2018.06.08  08-Jun-2018  2018.0  Unprovoked  AUSTRALIA  New South Wales   \n",
       "4  2018.06.04  04-Jun-2018  2018.0    Provoked     MEXICO           Colima   \n",
       "\n",
       "                         Location     Activity             Name Sex   \\\n",
       "0     Oceanside, San Diego County     Paddling      Julie Wolfe    F   \n",
       "1  St. Simon Island, Glynn County     Standing  AdysonÂ McNeely     F   \n",
       "2                    Habush, Oahu      Surfing      John Denges    M   \n",
       "3              Arrawarra Headland      Surfing             male    M   \n",
       "4                        La Ticla  Free diving   Gustavo Ramos     M   \n",
       "\n",
       "      ...             Species           Investigator or Source  \\\n",
       "0     ...          White shark                R. Collier, GSAF   \n",
       "1     ...                  NaN  K.McMurray, TrackingSharks.com   \n",
       "2     ...                  NaN  K.McMurray, TrackingSharks.com   \n",
       "3     ...            2 m shark                  B. Myatt, GSAF   \n",
       "4     ...      Tiger shark, 3m                       A .Kipper   \n",
       "\n",
       "                        pdf  \\\n",
       "0      2018.06.25-Wolfe.pdf   \n",
       "1    2018.06.18-McNeely.pdf   \n",
       "2     2018.06.09-Denges.pdf   \n",
       "3  2018.06.08-Arrawarra.pdf   \n",
       "4      2018.06.04-Ramos.pdf   \n",
       "\n",
       "                                        href formula  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "\n",
       "                                                href Case Number.1  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.25   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.18   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.09   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.08   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.04   \n",
       "\n",
       "  Case Number.2 original order Unnamed: 22 Unnamed: 23  \n",
       "0    2018.06.25         6303.0         NaN         NaN  \n",
       "1    2018.06.18         6302.0         NaN         NaN  \n",
       "2    2018.06.09         6301.0         NaN         NaN  \n",
       "3    2018.06.08         6300.0         NaN         NaN  \n",
       "4    2018.06.04         6299.0         NaN         NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25723, 24)\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates()\n",
    "\n",
    "\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't duplicates on the whole database, but these doesn't mean that we don't have duplicated columns. We will check these information later. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Action: Clean null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns with null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the column \"Unnamed: 22\" and \"Unnamed: 23\" only have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1478    stopped here\n",
      "Name: Unnamed: 22, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Unnamed: 22'].notnull().sum())\n",
    "\n",
    "print(df[df[\"Unnamed: 22\"].notnull()][\"Unnamed: 22\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4415             Teramo\n",
      "5840    change filename\n",
      "Name: Unnamed: 23, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['Unnamed: 23'].notnull().sum()\n",
    "\n",
    "print(df[df[\"Unnamed: 23\"].notnull()][\"Unnamed: 23\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see both columns don't have valuable data, so we can delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 22', 'Unnamed: 23'])\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean null rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to clean the rows, first we need to transpose index and columns. \n",
    "\n",
    "Then we will check those \"columns\" that are fully empty. Their sum of isnull will be 22, because this is the old number of columns that now are index.\n",
    "\n",
    "After identify these empty columns we will drop them from the dataFrame. \n",
    "\n",
    "When we get back to the clean dataFrame, where the index and columns are back to normal, we can see that acctually we have deleted the null rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8703, 22)\n"
     ]
    }
   ],
   "source": [
    "null_row = df.T.isnull().sum().values\n",
    "null_row_index_22 = df.T.isnull().sum()[df.T.isnull().sum() == 22].index\n",
    "clean_df = df.drop([e for e in null_row_index_22])\n",
    "\n",
    "\n",
    "#display(clean_df.head())\n",
    "print(clean_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third action: Clean some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check those columns that are not usefull for our analysis and delete them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicated columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the columns \"Case Number\", \"Case Number.1\" and \"Case Number.2\".\n",
    "\n",
    "1Âº Do an analysis apart from the dataFrame.\n",
    "\n",
    "2Âº Data wrangling - Change all the values of null to 0.\n",
    "\n",
    "3Âº See if they are equal(\"true\") or have diffent values(\"false\".\n",
    "\n",
    "4Âº With the result we saw that \"\"Case Number.1\" only differ on 20 values with \"Case Number.2\", which we can sat that they are particully the same. Now, when compare to \"Case Number\" they are 2.000 difent between them.\n",
    "\n",
    "5Âº Checking the different rows(those that have \"false\"). We can see that actually most of then are 0, the rest it is pretty much the same. This means we can delete the duplicate columns and mantain just the first one. \n",
    "\n",
    "6Âº Drop the columns on the dataFrame, not on the apart version we created for this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pandas/core/generic.py:5436: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "c_df = clean_df[['Case Number','Case Number.1', 'Case Number.2']]\n",
    "\n",
    "c_df['Case Number'].fillna(0, inplace = True) \n",
    "c_df['Case Number.1'].fillna(0, inplace = True) \n",
    "c_df['Case Number.2'].fillna(0, inplace = True)\n",
    "\n",
    "c_df['check'] = c_df['Case Number'] == c_df['Case Number.1']\n",
    "c_df['check2'] = c_df['Case Number.1'] == c_df['Case Number.2']\n",
    "\n",
    "#print(c_df[\"check\"].value_counts())\n",
    "#print(c_df[\"check2\"].value_counts())\n",
    "\n",
    "h = c_df[c_df['check'] == False][['Case Number','Case Number.1', 'Case Number.2']]\n",
    "#h.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop(columns=['Case Number.1', 'Case Number.2'])\n",
    "\n",
    "#clean_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns we won't need for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are columns that are too descritive or that have information that won't be usefull to create any analysis, so we can delete in order to have the database cleaner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>25-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Boating</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Paddling</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>N</td>\n",
       "      <td>18h00</td>\n",
       "      <td>White shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>18-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Standing</td>\n",
       "      <td>F</td>\n",
       "      <td>11</td>\n",
       "      <td>N</td>\n",
       "      <td>14h00  -15h00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>09-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>N</td>\n",
       "      <td>07h45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>08-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 m shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>04-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>Free diving</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tiger shark, 3m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Case Number         Date    Year        Type    Country             Area  \\\n",
       "0  2018.06.25  25-Jun-2018  2018.0     Boating        USA       California   \n",
       "1  2018.06.18  18-Jun-2018  2018.0  Unprovoked        USA          Georgia   \n",
       "2  2018.06.09  09-Jun-2018  2018.0     Invalid        USA           Hawaii   \n",
       "3  2018.06.08  08-Jun-2018  2018.0  Unprovoked  AUSTRALIA  New South Wales   \n",
       "4  2018.06.04  04-Jun-2018  2018.0    Provoked     MEXICO           Colima   \n",
       "\n",
       "      Activity Sex   Age Fatal (Y/N)           Time         Species   \n",
       "0     Paddling    F   57           N          18h00      White shark  \n",
       "1     Standing    F   11           N  14h00  -15h00              NaN  \n",
       "2      Surfing    M   48           N          07h45              NaN  \n",
       "3      Surfing    M  NaN           N            NaN        2 m shark  \n",
       "4  Free diving    M  NaN           N            NaN  Tiger shark, 3m  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = clean_df.drop(columns=['Location', 'Name','Injury', 'Investigator or Source', 'pdf', 'href formula', 'href', 'original order'])\n",
    "\n",
    "clean_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8703, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we only have 12 columns and the dataframe already looks much cleaner\n",
    "\n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four action: Clean by string matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will edit the data, so we group by on the analysis phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type: Only Unprovoked, Provoked and Not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unprovoked      4595\n",
       "Provoked         574\n",
       "Invalid          547\n",
       "Sea Disaster     239\n",
       "Boating          203\n",
       "Boat             137\n",
       "Questionable       2\n",
       "Boatomg            1\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unprovoked       5175\n",
       "Not Available    2954\n",
       "Provoked          574\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[\"Type\"] = clean_df.Type.fillna(\"Not Available\")\n",
    "\n",
    "\n",
    "clean_df.loc[clean_df[\"Type\"].str.startswith(\"B\"),\"Type\"] = \"Unprovoked\"\n",
    "clean_df.loc[clean_df[\"Type\"].str.startswith(\"S\"),\"Type\"] = \"Unprovoked\"\n",
    "clean_df.loc[clean_df[\"Type\"].str.startswith(\"I\"),\"Type\"] = \"Not Available\"\n",
    "clean_df.loc[clean_df[\"Type\"].str.startswith(\"Q\"),\"Type\"] = \"Not Available\"\n",
    "\n",
    "clean_df.Type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity - This will be one of the mains column for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Surfing         971\n",
       "Swimming        869\n",
       "Fishing         431\n",
       "Spearfishing    333\n",
       "Bathing         162\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.Activity.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggruping the activities on the key groups bellow, by changing the activity name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"Activity\"] = clean_df.Activity.fillna(\"Not Available\")\n",
    "\n",
    "#SURFING\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Surf\"),\"Activity\"] = \"Surfing\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Board\"),\"Activity\"] = \"Surfing\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"surf\"),\"Activity\"] = \"Surfing\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"board\"),\"Activity\"] = \"Surfing\"\n",
    "\n",
    "\n",
    "#Swimming\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Bath\"),\"Activity\"] = \"Swimming\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.startswith(\"Wad\"),\"Activity\"] = \"Swimming\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Swi\"),\"Activity\"] = \"Swimming\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.startswith(\"Floa\"),\"Activity\"] = \"Swimming\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"swi\"),\"Activity\"] = \"Swimming\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Jump\"),\"Activity\"] = \"Swimming\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Play\"),\"Activity\"] = \"Swimming\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Sculling\"),\"Activity\"] = \"Swimming\"\n",
    "\n",
    "\n",
    "#Fishing\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Fish\"),\"Activity\"] = \"Fishing\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"fish\"),\"Activity\"] = \"Fishing\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.startswith(\"Spear\"),\"Activity\"] = \"Fishing\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Clammin\"),\"Activity\"] = \"Fishing\"\n",
    "\n",
    "\n",
    "#Diving\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Div\"),\"Activity\"] = \"Diving\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"div\"),\"Activity\"] = \"Diving\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Snor\"),\"Activity\"] = \"Diving\"\n",
    "\n",
    "\n",
    "#Kayaking\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Canoe\"),\"Activity\"] = \"Kayaking\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Row\"),\"Activity\"] = \"Kayaking\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Kay\"),\"Activity\"] = \"Kayaking\"\n",
    "\n",
    "#Treading water\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Walk\"),\"Activity\"] = \"Treading water\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Dang\"),\"Activity\"] = \"Treading water\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Standing\"),\"Activity\"] = \"Treading water\"\n",
    "\n",
    "#Sailing\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Boat\"),\"Activity\"] = \"Sailing\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Ship\"),\"Activity\"] = \"Sailing\"\n",
    "\n",
    "#Fell\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Sea\"),\"Activity\"] = \"Fell into the water\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Fell\"),\"Activity\"] = \"Fell into the water\"\n",
    "\n",
    "#Interacting with sharks\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Shark\"),\"Activity\"] = \"Interacting with sharks\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"shark\"),\"Activity\"] = \"Interacting with sharks\"\n",
    "\n",
    "#Others\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Suicide\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Murder\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Paddleskiing\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Lifesaving drill\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Seine\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"SUP\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Splashing\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Escap\"),\"Activity\"] = \"Others\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.startswith(\".\"),\"Activity\"] = \"Not Available\"\n",
    "clean_df.loc[clean_df[\"Activity\"].str.contains(\"Escap\"),\"Activity\"] = \"Others\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All those activity that have less than 4 value_counts() can be deleted from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_Activity = clean_df.groupby(\"Activity\")[\"Activity\"].transform(len)\n",
    "\n",
    "mask = (counts_Activity > 5)\n",
    "\n",
    "#clean_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = clean_df[mask]\n",
    "\n",
    "#df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activity clean! This is super important because this is one of the columns that we will built our analysin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Available              2948\n",
       "Surfing                    1562\n",
       "Swimming                   1526\n",
       "Fishing                    1154\n",
       "Diving                      606\n",
       "Treading water              179\n",
       "Interacting with sharks     121\n",
       "Kayaking                     75\n",
       "Sailing                      51\n",
       "Fell into the water          45\n",
       "Others                       40\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex: M/F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M      4809\n",
       "F       606\n",
       "M         2\n",
       "N         2\n",
       ".         1\n",
       "lli       1\n",
       "Name: Sex , dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[\"Sex \"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/lib/python3/dist-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"Sex \"] = df_clean[\"Sex \"].fillna(\"Not Available\")\n",
    "\n",
    "df_clean.loc[df_clean[\"Sex \"].str.startswith(\"N\"),\"Sex \"] = \"M\"\n",
    "df_clean.loc[df_clean[\"Sex \"].str.startswith(\"M\"),\"Sex \"] = \"M\"\n",
    "df_clean.loc[df_clean[\"Sex \"].str.startswith(\".\"),\"Sex \"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Sex \"].str.startswith(\"O\"),\"Sex \"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Sex \"].str.startswith(\"l\"),\"Sex \"] = \"Not Available\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M                7699\n",
       "F                 606\n",
       "Not Available       2\n",
       "Name: Sex , dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sex is clean now!:\n",
    "\n",
    "df_clean[\"Sex \"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fatal: Y/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N          4127\n",
       "Y          1215\n",
       "UNKNOWN      68\n",
       " N            6\n",
       "N             1\n",
       "2017          1\n",
       "y             1\n",
       "M             1\n",
       "Name: Fatal (Y/N), dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[\"Fatal (Y/N)\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"Fatal (Y/N)\"] = df_clean[\"Fatal (Y/N)\"].fillna(\"Not Available\")\n",
    "\n",
    "df_clean.loc[df_clean[\"Fatal (Y/N)\"].str.startswith(\"U\"),\"Fatal (Y/N)\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Fatal (Y/N)\"].str.startswith(\"N\"),\"Fatal (Y/N)\"] = \"N\"\n",
    "df_clean.loc[df_clean[\"Fatal (Y/N)\"].str.startswith(\"y\"),\"Fatal (Y/N)\"] = \"Y\"\n",
    "df_clean.loc[df_clean[\"Fatal (Y/N)\"].str.startswith(\"M\"),\"Fatal (Y/N)\"] = \"N\"\n",
    "df_clean.loc[df_clean[\"Fatal (Y/N)\"].str.endswith(\"N\"),\"Fatal (Y/N)\"] = \"N\"\n",
    "df_clean.loc[df_clean[\"Fatal (Y/N)\"].str.startswith(\"2\"),\"Fatal (Y/N)\"] = \"N\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    7091\n",
       "Y    1216\n",
       "Name: Fatal (Y/N), dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[\"Fatal (Y/N)\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age: Let's arrange in Teen, Young, Adult and Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"Age\"] = df_clean[\"Age\"].fillna(\"0\")\n",
    "\n",
    "\n",
    "df_clean[\"Age\"]= df_clean[\"Age\"].astype(str) \n",
    "\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"1\"), \"Age\"] = \"Teen (1 - 19)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"2\"), \"Age\"] = \"Young (20 - 29)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"3\"), \"Age\"] = \"Adult (30 - 59)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"4\"), \"Age\"] = \"Adult (30 - 59)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"5\"), \"Age\"] = \"Adult (30 - 59)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"6\"), \"Age\"] = \"Old (> 60)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"7\"), \"Age\"] = \"Old (> 60)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"8\"), \"Age\"] = \"Old (> 60)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"9\"), \"Age\"] = \"Old (> 60)\"\n",
    "\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"een\"), \"Age\"] = \"Teen (1 - 19)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"oung\"), \"Age\"] = \"Young (20 - 29)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"dult\"), \"Age\"] = \"Adult (30 - 59)\"\n",
    "\n",
    "\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"mid\"), \"Age\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"X\"), \"Age\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"Ca\"), \"Age\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"F\"), \"Age\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"derly\"), \"Age\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"LINE\"), \"Age\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"A.M.\"), \"Age\"] = \"Not Available\"\n",
    "\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"50\"), \"Age\"] = \"Old (> 60)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"19\"), \"Age\"] = \"Teen (1 - 19)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"28\"), \"Age\"] = \"Young (20 - 29)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"14\"), \"Age\"] = \"Teen (1 - 19)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"43\"), \"Age\"] = \"Adult (30 - 59)\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.contains(\"30\"), \"Age\"] = \"Adult (30 - 59)\"\n",
    "\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\"0\"), \"Age\"] = \"Not Available\"\n",
    "df_clean.loc[df_clean[\"Age\"].str.startswith(\" \"), \"Age\"] = \"Not Available\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Available      4962\n",
       "Teen (1 - 19)      1082\n",
       "Adult (30 - 59)    1078\n",
       "Young (20 - 29)     980\n",
       "Old (> 60)          204\n",
       "Â                      1\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[\"Age\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column very important for our analysis. Will be difficult to clean all, but we can focus on the countries with most shark attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8307, 12)\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.shape)\n",
    "#df_clean[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"Country\"] = df_clean[\"Country\"].fillna(\"Not Available\")\n",
    "\n",
    "df_clean.loc[df_clean[\"Country\"].str.contains(\"USA\"), \"Country\"] = \"USA\"\n",
    "df_clean.loc[df_clean[\"Country\"].str.contains(\"AUST\"), \"Country\"] = \"AUSTRALIA\"\n",
    "df_clean.loc[df_clean[\"Country\"].str.contains(\"AFRICA\"), \"Country\"] = \"AFRICA\"\n",
    "df_clean.loc[df_clean[\"Country\"].str.contains(\"GUINEA\"), \"Country\"] = \"NEW GUINEA\"\n",
    "df_clean.loc[df_clean[\"Country\"].str.contains(\"PHILIP\"), \"Country\"] = \"PHILIPPINES\"\n",
    "df_clean.loc[df_clean[\"Country\"].str.contains(\"MEX\"), \"Country\"] = \"MEXICO\"\n",
    "df_clean.loc[df_clean[\"Country\"].str.contains(\"ITALY\"), \"Country\"] = \"ITALY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8307, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Not Available    2444\n",
       "USA              2144\n",
       "AUSTRALIA        1265\n",
       "AFRICA            559\n",
       "NEW GUINEA        136\n",
       "NEW ZEALAND       123\n",
       "BRAZIL            104\n",
       "BAHAMAS           102\n",
       "MEXICO             85\n",
       "ITALY              69\n",
       "Name: Country, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_clean.shape)\n",
    "df_clean[\"Country\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All those Countries that have less than 40 shark attack can be deleted from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_COUNTRY = df_clean.groupby(\"Country\")[\"Country\"].transform(len)\n",
    "\n",
    "Mask = (counts_COUNTRY > 40)\n",
    "\n",
    "#df_clean[Mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Available    2444\n",
       "USA              2144\n",
       "AUSTRALIA        1265\n",
       "AFRICA            559\n",
       "NEW GUINEA        136\n",
       "NEW ZEALAND       123\n",
       "BRAZIL            104\n",
       "BAHAMAS           102\n",
       "MEXICO             85\n",
       "ITALY              69\n",
       "FIJI               58\n",
       "REUNION            58\n",
       "NEW CALEDONIA      51\n",
       "SPAIN              44\n",
       "PHILIPPINES        43\n",
       "MOZAMBIQUE         42\n",
       "Name: Country, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_Clean = df_clean[Mask]\n",
    "\n",
    "DF_Clean[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, will be difficult to clean all these data because is too descritive, but we can clean up the top ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "White shark                                           141\n",
       "Invalid                                                77\n",
       "Shark involvement prior to death was not confirmed     77\n",
       "Shark involvement not confirmed                        76\n",
       "Tiger shark                                            56\n",
       "Name: Species , dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_Clean['Species '].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "DF_Clean['Species '] = DF_Clean['Species '].fillna(\"Not Available\")\n",
    "\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"hite\"), 'Species '] = \"White Shark\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"iger\"), 'Species '] = \"Tiger Shark\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"ull\"), 'Species '] = \"Bull Shark\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"obbe\"), 'Species '] = \"Wobbegong Shark\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"nvalid\"), 'Species '] = \"Not Available\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"uestiona\"), 'Species '] = \"Not Available\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"confirme\"), 'Species '] = \"Not Available\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"nvolvement\"), 'Species '] = \"Not Available\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"dentif\"), 'Species '] = \"Not Available\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"shark\"), 'Species '] = \"Not Available\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.contains(\"Blue\"), 'Species '] = \"Blue Shark\"\n",
    "DF_Clean.loc[DF_Clean['Species '].str.startswith(\" \"), 'Species '] = \"Not Available\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our database already looks much cleaner, but we still can shape it more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifty action: Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new column called Decada that we can use on the analysis. We will build based on the column Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0.0        56\n",
       "5.0         1\n",
       "77.0        1\n",
       "500.0       1\n",
       "1555.0      1\n",
       "1638.0      1\n",
       "1642.0      2\n",
       "1721.0      1\n",
       "1738.0      1\n",
       "1742.0      1\n",
       "1751.0      1\n",
       "1764.0      1\n",
       "1771.0      1\n",
       "1776.0      1\n",
       "1779.0      1\n",
       "1780.0      1\n",
       "1787.0      1\n",
       "1788.0      1\n",
       "1791.0      1\n",
       "1797.0      1\n",
       "1801.0      1\n",
       "1803.0      2\n",
       "1804.0      1\n",
       "1805.0      1\n",
       "1807.0      1\n",
       "1810.0      1\n",
       "1811.0      1\n",
       "1816.0      1\n",
       "1817.0      1\n",
       "1819.0      1\n",
       "         ... \n",
       "1989.0     52\n",
       "1990.0     38\n",
       "1991.0     35\n",
       "1992.0     48\n",
       "1993.0     39\n",
       "1994.0     50\n",
       "1995.0     65\n",
       "1996.0     55\n",
       "1997.0     52\n",
       "1998.0     62\n",
       "1999.0     55\n",
       "2000.0     85\n",
       "2001.0     83\n",
       "2002.0     78\n",
       "2003.0     85\n",
       "2004.0     79\n",
       "2005.0     87\n",
       "2006.0     97\n",
       "2007.0     99\n",
       "2008.0    107\n",
       "2009.0     97\n",
       "2010.0     76\n",
       "2011.0    104\n",
       "2012.0    106\n",
       "2013.0    106\n",
       "2014.0    114\n",
       "2015.0    131\n",
       "2016.0    116\n",
       "2017.0    116\n",
       "2018.0     45\n",
       "Name: Year, Length: 217, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_Clean.Year.value_counts().reset_index().sort_values(\"index\").set_index('index').Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "DF_Clean[\"Decada\"] = pd.cut(DF_Clean.Year,\n",
    "                                bins=[1949,1959,1969,1979,1989,1999,2009,2019], \n",
    "                                labels=range(1950,2020,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010    914\n",
       "2000    897\n",
       "1990    499\n",
       "1960    467\n",
       "1980    381\n",
       "1950    315\n",
       "1970    271\n",
       "Name: Decada, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_Clean[\"Decada\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Clean Actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the columns we won't use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we saw the date, we know better those columns that we can delete, because we won't use on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Clean = DF_Clean.drop(columns=['Case Number','Date','Area','Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Clean['Year'] = DF_Clean['Year'].fillna(\"Not Available\")\n",
    "DF_Clean['Type'] = DF_Clean['Type'].fillna(\"Not Available\")\n",
    "DF_Clean['Country'] = DF_Clean['Country'].fillna(\"Not Available\")\n",
    "DF_Clean['Activity'] = DF_Clean['Activity'].fillna(\"Not Available\")\n",
    "DF_Clean['Sex '] = DF_Clean['Sex '].fillna(\"Not Available\")\n",
    "DF_Clean['Age'] = DF_Clean['Age'].fillna(\"Not Available\")\n",
    "DF_Clean['Fatal (Y/N)'] = DF_Clean['Fatal (Y/N)'].fillna(\"Not Available\")\n",
    "DF_Clean['Species '] = DF_Clean['Species '].fillna(\"Not Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop \"Not Available\" in the column Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information Country is essential for our analysis, so if we don't have this information we can disregard from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Clean = DF_Clean.drop(DF_Clean[DF_Clean['Country']==\"Not Available\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all \"NaN\" from the column Decada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Decada is NaN it means that this information os either from before 1949 or because the data Year was not fulfill corrected. So, we can drop these data because is useless.\n",
    "\n",
    "We can use the formula below straight on the database, because the only column that have \"NaN\" is Decada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Clean = DF_Clean.replace(-1, np.NaN).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Again with we have duplicated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we already done cleaning the database we can see if we have any duplicated row and in case yes, we can delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Species</th>\n",
       "      <th>Decada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Treading water</td>\n",
       "      <td>F</td>\n",
       "      <td>Teen (1 - 19)</td>\n",
       "      <td>N</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>USA</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>Adult (30 - 59)</td>\n",
       "      <td>N</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>N</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Diving</td>\n",
       "      <td>M</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>N</td>\n",
       "      <td>Tiger Shark</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>N</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year           Type    Country        Activity Sex               Age  \\\n",
       "1  2018     Unprovoked        USA  Treading water    F    Teen (1 - 19)   \n",
       "2  2018  Not Available        USA         Surfing    M  Adult (30 - 59)   \n",
       "3  2018     Unprovoked  AUSTRALIA         Surfing    M    Not Available   \n",
       "4  2018       Provoked     MEXICO          Diving    M    Not Available   \n",
       "5  2018     Unprovoked  AUSTRALIA         Surfing    M    Not Available   \n",
       "\n",
       "  Fatal (Y/N)       Species  Decada  \n",
       "1           N  Not Available   2010  \n",
       "2           N  Not Available   2010  \n",
       "3           N  Not Available   2010  \n",
       "4           N    Tiger Shark   2010  \n",
       "5           N  Not Available   2010  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3732, 9)\n"
     ]
    }
   ],
   "source": [
    "DF_Clean.drop_duplicates()\n",
    "\n",
    "display(DF_Clean.head())\n",
    "print(DF_Clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Base clean!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started: 25.723 X 24\n",
    "\n",
    "Finished: 3.732 X 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the new database to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Clean.to_csv(r\"/home/carolina/Desktop/IRONHACK/Project files/clean_df_attacks.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
